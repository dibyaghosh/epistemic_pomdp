
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/dmhendricks/bootstrap-grid-css@4.1.3/dist/css/bootstrap-grid.min.css" />

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
  }


  h1 {
    font-weight:300;
  }


  h2 {
    font-weight:300;
  }
  .author {
      font-size: 1.5em;
      text-align: center;
      padding-top: 0.5em;
      padding-bottom: 0.5em;
  }

  #teaser {
    max-width: 600px;
  }

  @media (max-width: 768px) {
  #teaser {
    max-width: 100%;
  }
}


  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }
  p {
  padding-top: 1em;
  padding-bottom: 1em;
  font-size: 1em;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    margin: 1em;
  }
</style>


<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/png" href="https://dibyaghosh.com/images/research/dnc.png">
    <title>Divide-and-Conquer Reinforcement Learning</title>
    <meta property='og:title' content='Divide-and-Conquer Reinforcement Learning' />
    <meta property="og:description" content="Ghosh, Singh, Rajeswaran, Kumar, Levine. Divide-and-Conquer Reinforcement Learning. In ICLR, 2018." />
    <meta property='og:url' content='https://dibyaghosh.com/dnc' />
    <meta property='og:video' content='dnc_video.mp4' />
    <meta property='og:image' content='https://dibyaghosh.com/images/research/dnc.png' />
  </head>

  <body>
    <div class="bootstrap-wrapper">
      <div class="container">
        <header>
        <h1><center><span style="font-size:1.5em;font-weight:bold;">Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability </span></center></h1>
        </header>
        <div class="row">
            <div class="col author"><a href="https://dibyaghosh.com" target="_blank">Dibya Ghosh<sup>1</sup></a></div>
            <div class="col author"><a href="https://twitter.com/jrahme0?lang=en" target="_blank">Jad Rahme<sup>2</sup></a></div>
            <div class="col author"><a href="https://aviralkumar2907.github.io/" target="_blank">Aviral Kumar<sup>1</sup></a></div>
            <div class="col author"><a href="https://amyzhang.github.io/" target="_blank">Amy Zhang<sup>1, 3</sup></a></div>
            <div class="col author"><a href="https://www.cs.princeton.edu/~rpa/" target="_blank">Ryan P. Adams<sup>2</sup></a></div>
            <div class="col author"><a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine<sup>1</sup></a></div>

        </div>
        <div class="row">
            <div class="col author"><sup>1</sup>University of California, Berkeley</div>
            <div class="col author"><sup>2</sup>Princeton University</div>
            <div class="col author"><sup>3</sup>Facebook AI Research</div>
        </div>

            <div class="author"><center><a href="https://neurips.cc/Conferences/2021" target="_blank">NeurIPS 2021</a></center></div>
            <div class="author"><a href="https://arxiv.org/abs/2107.06277">[Paper]</a>
                 <!-- <a href="https://github.com/dibyaghosh/dnc">[Code]</a></div> -->

        <center id="demoVideo"><h1>Video</h1>

        <iframe width="560" height="315" src="https://www.youtube.com/embed/mOuf2fvNAhY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <br />
    </center>
<p>
    Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world.
    In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization
     beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize
     effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL.
     We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite.

        </p>
      <!-- <hr>

            <center><a href="teaser.png"><img id="teaser" src="teaser.png"></img></a><br></center>
        <center id="sourceCode"><h1>High Level Idea</h1></center>
        <p>
        For problems with high initial state variability, we partition the initial state space of the task into contexts using clustering algorithms. We train an ensemble of <i> local </i> policies, each specialized to a particular context. The objective for each policy is to maximize expected reward and minimize the KL divergence from each of the other policies in the ensemble. On a slower timescale, the ensemble is distilled together into a <i> global </i> policy, which can operate on
        the original task. 
      </p>

      <hr>

      <center><h1>Paper</h1></center>
      <div class="row">
        <div class="col-12 col-md-6">
          <p>
            <div><a href="https://arxiv.org/pdf/1711.09874.pdf"><img style="max-height:200px; max-width: 100%;" src="preview.jpg"/></a></div>
            <div class="author"><a href="https://arxiv.org/pdf/1711.09874.pdf">[Paper PDF]</a>&nbsp;<a href="https://arxiv.org/abs/1711.09874">[arXiv]</a></div>
          </p>
          </div>
        <div class="col-12 col-md-6">
            <p style="text-align:left;">
              <b><span style="font-size:2em">Citation</span></b><br/><span style="font-size:1em;">&nbsp;<br/></span>
              <span style="font-size:1.3em">Dibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, Sergey Levine <br/> <b>Divide-and-Conquer Reinforcement Learning</b> In <i>ICLR 2018</i>.</span></p>

        </div>
      </div> -->
</div>
</div>

</body>
</html>